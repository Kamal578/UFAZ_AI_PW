{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 3 - Entropy of a string and Huffman coding\n",
        "## Authors:\n",
        "|Name|Surname|Student ID|E-mail|\n",
        "|---|---|---|---|\n",
        "|Kamal|Ahmadov|22022692|kamal.ahmadov1@ufaz.az|   \n",
        "|Murad|Mustafayev|22022733 |murad.mustafayev@ufaz.az|"
      ],
      "metadata": {
        "id": "I4V_J5PDWHKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Entropy of a character string\n",
        "Recall the formula presented during the lecture for computing the entropy of a character string:\n",
        "$$H(f) = - \\sum_{v=1}^{n} p(v) \\log_2(p(v))$$\n",
        "where $v$ is a byte of the string and $p(v)$ is its probability of being in the string.\n",
        "\n",
        "## 1.1 Implement a method to compute and store the occurences of each character in a string\n"
      ],
      "metadata": {
        "id": "htF2Ko2rvcE_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qbOWhkWsVQit"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import heapq\n",
        "from collections import Counter, defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_occurrences(string):\n",
        "    \"\"\"\n",
        "    Compute the occurrences of each character in the given string.\n",
        "\n",
        "    Args:\n",
        "        string (str): The input string.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are characters and values are their occurrences.\n",
        "        int: Total number of characters in the string.\n",
        "    \"\"\"\n",
        "    occurrences = {}\n",
        "    total_chars = len(string)\n",
        "\n",
        "    # Iterate over each character in the string\n",
        "    for char in string:\n",
        "        # Update the occurrences dictionary\n",
        "        if char in occurrences:\n",
        "            occurrences[char] += 1\n",
        "        else:\n",
        "            occurrences[char] = 1\n",
        "\n",
        "    return occurrences, total_chars"
      ],
      "metadata": {
        "id": "VCBd9Blwv4IO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2  Implement a method to compute the entropy of a character string."
      ],
      "metadata": {
        "id": "f5wG68kKwAJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_entropy(string):\n",
        "    \"\"\"\n",
        "    Compute the entropy of the given string.\n",
        "\n",
        "    Args:\n",
        "        string (str): The input string.\n",
        "\n",
        "    Returns:\n",
        "        float: The entropy value of the string.\n",
        "    \"\"\"\n",
        "    # Compute the occurrences of each character and the total number of characters\n",
        "    occurrences, total_chars = compute_occurrences(string)\n",
        "    entropy = 0.0\n",
        "\n",
        "    # Calculate the entropy using the formula H(f) = - \\sum_{v=1}^{n} p(v) \\log_2(p(v))\n",
        "    for char, count in occurrences.items():\n",
        "        probability = count / total_chars\n",
        "        entropy -= probability * math.log2(probability)\n",
        "\n",
        "    return entropy"
      ],
      "metadata": {
        "id": "udIncSZDwErR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the functions\n",
        "test_string = \"hello world\"\n",
        "entropy = compute_entropy(test_string)\n",
        "print(\"Entropy of '{}' is: {:.4f}\".format(test_string, entropy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRuEnR54wK-J",
        "outputId": "ef492449-c70a-42c6-8243-d050312e5d57"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entropy of 'hello world' is: 2.8454\n",
            "Entropy of 'she sells seashells by the seashore' is: 3.0361\n",
            "Entropy of 'peter piper picked a peck of pickled peppers' is: 3.3412\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.  Huffman coding\n",
        "\n",
        "## 2.1 Design a class `Node` that represents a node in a binary tree: a node is characterized by a value, a left child and a right child. If the node is a leaf, then both children are `null`."
      ],
      "metadata": {
        "id": "savpwP19wjYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, character, frequency):\n",
        "        self.character = character\n",
        "        self.frequency = frequency\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "\n",
        "    def __lt__(self, other):\n",
        "        return self.frequency < other.frequency\n"
      ],
      "metadata": {
        "id": "OuxksPAQwvdQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Implement a method to compute the optimal tree to encode given character string."
      ],
      "metadata": {
        "id": "hSQ-7nCTw03m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_huffman_tree(text):\n",
        "    frequency_map = defaultdict(int)\n",
        "    for c in text:\n",
        "        frequency_map[c] += 1\n",
        "\n",
        "    priority_queue = []\n",
        "    for char, freq in frequency_map.items():\n",
        "        heapq.heappush(priority_queue, Node(char, freq))\n",
        "\n",
        "    while len(priority_queue) > 1:\n",
        "        left = heapq.heappop(priority_queue)\n",
        "        right = heapq.heappop(priority_queue)\n",
        "\n",
        "        parent = Node('\\0', left.frequency + right.frequency)\n",
        "        parent.left = left\n",
        "        parent.right = right\n",
        "        heapq.heappush(priority_queue, parent)\n",
        "\n",
        "    return heapq.heappop(priority_queue)\n",
        "\n",
        "def compute_average_bits(root):\n",
        "    return compute_average_bits_helper(root, 0, 0) / root.frequency\n",
        "\n",
        "def compute_average_bits_helper(node, depth, path_length):\n",
        "    if node is None:\n",
        "        return 0\n",
        "    if node.left is None and node.right is None:\n",
        "        return depth * node.frequency\n",
        "    return compute_average_bits_helper(node.left, depth + 1, path_length + 1) + \\\n",
        "           compute_average_bits_helper(node.right, depth + 1, path_length + 1)\n",
        "\n",
        "def build_encoding_map(node, encoding, encoding_map):\n",
        "    if node is None:\n",
        "        return\n",
        "    if node.left is None and node.right is None:\n",
        "        encoding_map[node.character] = encoding\n",
        "        return\n",
        "    build_encoding_map(node.left, encoding + '0', encoding_map)\n",
        "    build_encoding_map(node.right, encoding + '1', encoding_map)\n",
        "\n",
        "def encode_string(text, root):\n",
        "    encoding_map = {}\n",
        "    build_encoding_map(root, \"\", encoding_map)\n",
        "\n",
        "    encoded_string = \"\"\n",
        "    for c in text:\n",
        "        encoded_string += encoding_map[c]\n",
        "    return encoded_string\n",
        "\n",
        "def decode_string(encoded_string, root):\n",
        "    decoded_string = \"\"\n",
        "    current = root\n",
        "    for bit in encoded_string:\n",
        "        if bit == '0':\n",
        "            current = current.left\n",
        "        elif bit == '1':\n",
        "            current = current.right\n",
        "        if current.left is None and current.right is None:\n",
        "            decoded_string += current.character\n",
        "            current = root\n",
        "    return decoded_string"
      ],
      "metadata": {
        "id": "u5zQHVNeevOU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.(a) Test your method with the following sentences: “peter piper picked a peck of pickled peppers” and “she sells seashells by the seashore”\n"
      ],
      "metadata": {
        "id": "dP03CDnQe2e7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = \"peter piper picked a peck of pickled peppers\"\n",
        "tree1 = build_huffman_tree(sentence1)\n",
        "average_bits1 = compute_average_bits(tree1)\n",
        "encoded1 = encode_string(sentence1, tree1)\n",
        "decoded1 = decode_string(encoded1, tree1)\n",
        "entropy1 = compute_entropy(sentence1)\n",
        "\n",
        "print(\"\\nSentence 1:\")\n",
        "print(\"Average Bits per Character: {:.4f}\".format(average_bits1))\n",
        "print(\"Entropy: {:.4f}\".format(entropy1))\n",
        "print(\"Encoded String: {}\".format(encoded1))\n",
        "print(\"Decoded String: {}\".format(decoded1))\n",
        "\n",
        "sentence2 = \"she sells seashells by the seashore\"\n",
        "tree2 = build_huffman_tree(sentence2)\n",
        "average_bits2 = compute_average_bits(tree2)\n",
        "encoded2 = encode_string(sentence2, tree2)\n",
        "decoded2 = decode_string(encoded2, tree2)\n",
        "entropy2 = compute_entropy(sentence2)\n",
        "\n",
        "print(\"\\nSentence 2:\")\n",
        "print(\"Average Bits per Character: {:.4f}\".format(average_bits2))\n",
        "print(\"Entropy: {:.4f}\".format(entropy2))\n",
        "print(\"Encoded String: {}\".format(encoded2))\n",
        "print(\"Decoded String: {}\".format(decoded2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDCnplzne6Xr",
        "outputId": "e3bef871-45a0-420b-d645-88d8fdc85e2a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence 1:\n",
            "Average Bits per Character: 3.3864\n",
            "Entropy: 3.3412\n",
            "Encoded String: 01111001001111000110011011011111000110011011101010011110000110000101100111110101001110000110010111001101110101001001111110000110011110101111100000110\n",
            "Decoded String: peter piper picked a peck of pickled peppers\n",
            "\n",
            "Sentence 2:\n",
            "Average Bits per Character: 3.0857\n",
            "Entropy: 3.0361\n",
            "Encoded String: 100011111101011101101110110101110001100011110110111011001000010011100000001111110101110001100010101001011111\n",
            "Decoded String: she sells seashells by the seashore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We notice that for both sentences, the calculated entropy is almost the same as the average number of bits per character.\n",
        "\n",
        "\n",
        "Huffman coding is a smart way to compress data by giving shorter codes to characters that appear more often. When we calculate the average number of bits used per character, it's very close to the entropy of the string. This similarity shows how Huffman coding effectively reduces the average number of bits needed for encoding."
      ],
      "metadata": {
        "id": "-FDP7GZk-9K_"
      }
    }
  ]
}